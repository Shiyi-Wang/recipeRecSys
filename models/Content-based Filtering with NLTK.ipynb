{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Content-based Filtering with NLTK\n",
        "Author: Taichang Zhou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzaYjYfdza7b",
        "outputId": "96c17123-68b9-457c-9c1d-88058e8e9b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (1.1.4)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask) (2.0.1)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from redis) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.7/dist-packages (from redis) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis) (4.11.3)\n",
            "Requirement already satisfied: deprecated>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from redis) (1.2.13)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from redis) (4.0.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis) (1.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.4->redis) (3.0.7)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: gensim in /root/.local/lib/python3.7/site-packages (4.1.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "%pip install unidecode\n",
        "%pip install flask\n",
        "%pip install redis\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "%load_ext tensorboard\n",
        "%pip install --upgrade gensim --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtU0CvRhyxEV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import nltk\n",
        "import string\n",
        "import ast\n",
        "import re\n",
        "import unidecode\n",
        "# nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import time\n",
        "import redis\n",
        "from flask import current_app\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from gensim.models import Word2Vec\n",
        "import io\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ohNyZArzser"
      },
      "outputs": [],
      "source": [
        "def ingredient_parser(ingreds):\n",
        "    '''\n",
        "    \n",
        "    This function takes in a list (but it is a string as it comes from pandas dataframe) of \n",
        "       ingredients and performs some preprocessing. \n",
        "       For example:\n",
        "\n",
        "       input = '['1 x 1.6kg whole duck', '2 heaped teaspoons Chinese five-spice powder', '1 clementine',\n",
        "                 '6 fresh bay leaves', 'GRAVY', '', '1 bulb of garlic', '2 carrots', '2 red onions', \n",
        "                 '3 tablespoons plain flour', '100 ml Marsala', '1 litre organic chicken stock']'\n",
        "       \n",
        "       output = ['duck', 'chinese five spice powder', 'clementine', 'fresh bay leaf', 'gravy', 'garlic',\n",
        "                 'carrot', 'red onion', 'plain flour', 'marsala', 'organic chicken stock']\n",
        "\n",
        "    '''\n",
        "    measures = ['teaspoon', 't', 'tsp.', 'tablespoon', 'T', 'tbl.', 'tb', 'tbsp.', 'fluid ounce', 'fl oz', 'gill', 'cup', 'c', 'pint', 'p', 'pt', 'fl pt', 'quart', 'q', 'qt', 'fl qt', 'gallon', 'g', 'gal', 'ml', 'milliliter', 'millilitre', 'cc', 'mL', 'l', 'liter', 'litre', 'L', 'dl', 'deciliter', 'decilitre', 'dL', 'bulb', 'level', 'heaped', 'rounded', 'whole', 'pinch', 'medium', 'slice', 'pound', 'lb', '#', 'ounce', 'oz', 'mg', 'milligram', 'milligramme', 'g', 'gram', 'gramme', 'kg', 'kilogram', 'kilogramme', 'x', 'of', 'mm', 'millimetre', 'millimeter', 'cm', 'centimeter', 'centimetre', 'm', 'meter', 'metre', 'inch', 'in', 'milli', 'centi', 'deci', 'hecto', 'kilo']\n",
        "    words_to_remove = ['fresh', 'oil', 'a', 'red', 'bunch', 'and', 'clove', 'or', 'leaf', 'chilli', 'large', 'extra', 'sprig', 'ground', 'handful', 'free', 'small', 'pepper', 'virgin', 'range', 'from', 'dried', 'sustainable', 'black', 'peeled', 'higher', 'welfare', 'seed', 'for', 'finely', 'freshly', 'sea', 'quality', 'white', 'ripe', 'few', 'piece', 'source', 'to', 'organic', 'flat', 'smoked', 'ginger', 'sliced', 'green', 'picked', 'the', 'stick', 'plain', 'plus', 'mixed', 'mint', 'bay', 'basil', 'your', 'cumin', 'optional', 'fennel', 'serve', 'mustard', 'unsalted', 'baby', 'paprika', 'fat', 'ask', 'natural', 'skin', 'roughly', 'into', 'such', 'cut', 'good', 'brown', 'grated', 'trimmed', 'oregano', 'powder', 'yellow', 'dusting', 'knob', 'frozen', 'on', 'deseeded', 'low', 'runny', 'balsamic', 'cooked', 'streaky', 'nutmeg', 'sage', 'rasher', 'zest', 'pin', 'groundnut', 'breadcrumb', 'turmeric', 'halved', 'grating', 'stalk', 'light', 'tinned', 'dry', 'soft', 'rocket', 'bone', 'colour', 'washed', 'skinless', 'leftover', 'splash', 'removed', 'dijon', 'thick', 'big', 'hot', 'drained', 'sized', 'chestnut', 'watercress', 'fishmonger', 'english', 'dill', 'caper', 'raw', 'worcestershire', 'flake', 'cider', 'cayenne', 'tbsp', 'leg', 'pine', 'wild', 'if', 'fine', 'herb', 'almond', 'shoulder', 'cube', 'dressing', 'with', 'chunk', 'spice', 'thumb', 'garam', 'new', 'little', 'punnet', 'peppercorn', 'shelled', 'saffron', 'other''chopped', 'salt', 'olive', 'taste', 'can', 'sauce', 'water', 'diced', 'package', 'italian', 'shredded', 'divided', 'parsley', 'vinegar', 'all', 'purpose', 'crushed', 'juice', 'more', 'coriander', 'bell', 'needed', 'thinly', 'boneless', 'half', 'thyme', 'cubed', 'cinnamon', 'cilantro', 'jar', 'seasoning', 'rosemary', 'extract', 'sweet', 'baking', 'beaten', 'heavy', 'seeded', 'tin', 'vanilla', 'uncooked', 'crumb', 'style', 'thin', 'nut', 'coarsely', 'spring', 'chili', 'cornstarch', 'strip', 'cardamom', 'rinsed', 'honey', 'cherry', 'root', 'quartered', 'head', 'softened', 'container', 'crumbled', 'frying', 'lean', 'cooking', 'roasted', 'warm', 'whipping', 'thawed', 'corn', 'pitted', 'sun', 'kosher', 'bite', 'toasted', 'lasagna', 'split', 'melted', 'degree', 'lengthwise', 'romano', 'packed', 'pod', 'anchovy', 'rom', 'prepared', 'juiced', 'fluid', 'floret', 'room', 'active', 'seasoned', 'mix', 'deveined', 'lightly', 'anise', 'thai', 'size', 'unsweetened', 'torn', 'wedge', 'sour', 'basmati', 'marinara', 'dark', 'temperature', 'garnish', 'bouillon', 'loaf', 'shell', 'reggiano', 'canola', 'parmigiano', 'round', 'canned', 'ghee', 'crust', 'long', 'broken', 'ketchup', 'bulk', 'cleaned', 'condensed', 'sherry', 'provolone', 'cold', 'soda', 'cottage', 'spray', 'tamarind', 'pecorino', 'shortening', 'part', 'bottle', 'sodium', 'cocoa', 'grain', 'french', 'roast', 'stem', 'link', 'firm', 'asafoetida', 'mild', 'dash', 'boiling']\n",
        "    # The ingredient list is now a string so we need to turn it back into a list. We use ast.literal_eval\n",
        "    \n",
        "    if isinstance(ingreds, list):\n",
        "        ingredients = ingreds\n",
        "    else:\n",
        "        ingredients = ast.literal_eval(ingreds)\n",
        "    # We first get rid of all the punctuation. We make use of str.maketrans. It takes three input \n",
        "    # arguments 'x', 'y', 'z'. 'x' and 'y' must be equal-length strings and characters in 'x'\n",
        "    # are replaced by characters in 'y'. 'z' is a string (string.punctuation here) where each character\n",
        "    #  in the string is mapped to None. \n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    ingred_list = []\n",
        "    for i in ingredients:\n",
        "        i.translate(translator)\n",
        "        # We split up with hyphens as well as spaces\n",
        "        items = re.split(' |-', i)\n",
        "        # Get rid of words containing non alphabet letters\n",
        "        items = [word for word in items if word.isalpha()]\n",
        "        # Turn everything to lowercase\n",
        "        items = [word.lower() for word in items]\n",
        "        # remove accents\n",
        "        items = [unidecode.unidecode(word) for word in items] #''.join((c for c in unicodedata.normalize('NFD', items) if unicodedata.category(c) != 'Mn'))\n",
        "        # Lemmatize words so we can compare words to measuring words\n",
        "        items = [lemmatizer.lemmatize(word) for word in items]\n",
        "        # Gets rid of measuring words/phrases, e.g. heaped teaspoon\n",
        "        items = [word for word in items if word not in measures]\n",
        "        # Get rid of common easy words\n",
        "        items = [word for word in items if word not in words_to_remove]\n",
        "        if items:\n",
        "            \n",
        "            ingred_list.append(' '.join(items)) \n",
        "    \n",
        "    return ingred_list\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vIqhlc4ci2-"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms-Xz18iGBoF",
        "outputId": "f84d313a-905d-48f9-e3f6-707dfffcf2ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of corpus: 231637\n"
          ]
        }
      ],
      "source": [
        "# get corpus with the documents sorted in alphabetical order\n",
        "from pathlib import Path  \n",
        "def get_and_sort_corpus(data):\n",
        "    corpus_sorted = []\n",
        "    for doc in data.parsed.values:\n",
        "        doc.sort()\n",
        "        corpus_sorted.append(doc)\n",
        "    return corpus_sorted\n",
        "  \n",
        "# calculate average length of each document \n",
        "def get_window(corpus):\n",
        "    lengths = [len(doc) for doc in corpus]\n",
        "    avg_len = float(sum(lengths)) / len(lengths)\n",
        "    return round(avg_len)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # load in data\n",
        "    data = pd.read_csv('/content/drive/MyDrive/RAW_recipes.csv')\n",
        "    # parse the ingredients for each recipe\n",
        "    data['parsed'] = data.ingredients.apply(ingredient_parser)\n",
        "    \n",
        "    # get corpus\n",
        "    corpus = get_and_sort_corpus(data)\n",
        "    print(f\"Length of corpus: {len(corpus)}\")\n",
        "    # train and save CBOW Word2Vec model\n",
        "    model_cbow = Word2Vec(\n",
        "      corpus, sg=0, workers=1, window=get_window(corpus), min_count=1, \n",
        "    )\n",
        "    filepath = Path('/content/drive/MyDrive/model_cbow.model')  \n",
        "    filepath.parent.mkdir(parents=True, exist_ok=True) \n",
        "    MODELPATH='/content/drive/MyDrive/model_cbow.model'\n",
        "    if model_cbow.save('model_cbow.model'):\n",
        "      print(\"Word2Vec model successfully trained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1Qe0u9PQaxb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path  \n",
        "filepath = Path('/content/drive/MyDrive/RAW_recipes_parsed.csv')  \n",
        "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
        "PAERSEPATH='/content/drive/MyDrive/RAW_recipes_parsed.csv'\n",
        "\n",
        "if data.to_csv(filepath):\n",
        "  print('saved')  \n",
        "isdf=pd.read_csv(PAERSEPATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AqsFWAZOrL9"
      },
      "outputs": [],
      "source": [
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, model_cbow):\n",
        "        self.model_cbow = model_cbow\n",
        "        self.vector_size = model_cbow.wv.vector_size\n",
        "\n",
        "    def fit(self):  \n",
        "        return self\n",
        "\n",
        "    def transform(self, docs): \n",
        "        doc_vector = self.doc_average_list(docs)\n",
        "        return doc_word_vector\n",
        "\n",
        "    def doc_average(self, doc):\n",
        "        mean = []\n",
        "        for word in doc:\n",
        "            if word in self.model_cbow.wv.index_to_key:\n",
        "                mean.append(self.model_cbow.wv.get_vector(word))\n",
        "\n",
        "        if not mean: \n",
        "            return np.zeros(self.vector_size)\n",
        "        else:\n",
        "            mean = np.array(mean).mean(axis=0)\n",
        "            return mean\n",
        "\n",
        "    def doc_average_list(self, docs):\n",
        "        return np.vstack([self.doc_average(doc) for doc in docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOVmJZlaOuwl"
      },
      "outputs": [],
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "    def __init__(self, model_cbow):\n",
        "\n",
        "        self.model_cbow = model_cbow\n",
        "        self.word_idf_weight = None\n",
        "        self.vector_size = model_cbow.wv.vector_size\n",
        "\n",
        "    def fit(self, docs): \n",
        "        \"\"\"\n",
        "\tBuild a tfidf model to compute each word's idf as its weight.\n",
        "\t\"\"\"\n",
        "\t\n",
        "        text_docs = []\n",
        "        for doc in docs:\n",
        "            text_docs.append(\" \".join(doc))\n",
        "\n",
        "        tfidf = TfidfVectorizer()\n",
        "        tfidf.fit(text_docs)  \n",
        "        # if a word was never seen it is given idf of the max of known idf value\n",
        "        max_idf = max(tfidf.idf_)  \n",
        "        self.word_idf_weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()],\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def transform(self, docs): \n",
        "        doc_word_vector = self.doc_average_list(docs)\n",
        "        return doc_word_vector\n",
        "\n",
        "    def doc_average(self, doc):\n",
        "\n",
        "\t\n",
        "        mean = []\n",
        "        for word in doc:\n",
        "            if word in self.model_cbow.wv.index_to_key:\n",
        "                mean.append(\n",
        "                    self.model_cbow.wv.get_vector(word) * self.word_idf_weight[word]\n",
        "                ) \n",
        "\n",
        "        if not mean:  \n",
        "            return np.zeros(self.vector_size)\n",
        "        else:\n",
        "            mean = np.array(mean).mean(axis=0)\n",
        "            return mean\n",
        "\n",
        "    def doc_average_list(self, docs):\n",
        "        return np.vstack([self.doc_average(doc) for doc in docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuHWw14LOvhH",
        "outputId": "29252696-730f-4e1b-cac5-0be1868d84a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded model\n",
            "                                       recipe  \\\n",
            "0  arriba   baked winter squash mexican style   \n",
            "1      squashed squash   hubbard or butternut   \n",
            "2                  chantel s vegetarian chili   \n",
            "3                         three sisters  stew   \n",
            "4        north carolina style tempeh barbecue   \n",
            "\n",
            "                                         ingredients               score  url  \\\n",
            "0  winter squash,mexican seasoning,mixed spice,ho...  0.9999999999999974  NaN   \n",
            "1  winter squash,unsalted butter,nutmeg,salt and ...  0.9019273284036903  NaN   \n",
            "2  olive oil,jalapeno pepper,white onion,cumin,ch...  0.8908118099457888  NaN   \n",
            "3  sugar pumpkin,olive oil,onion,garlic cloves,gr...  0.8754145760506676  NaN   \n",
            "4  tempeh,vegetable oil,onion,chipotle pepper,cid...  0.8718694971322888  NaN   \n",
            "\n",
            "         id  \n",
            "0  137739.0  \n",
            "1   35397.0  \n",
            "2   42195.0  \n",
            "3  261482.0  \n",
            "4  112444.0  \n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def ingredient_parser_final(ingredient):\n",
        "    if isinstance(ingredient, list):\n",
        "        ingredients = ingredient\n",
        "    else:\n",
        "        ingredients = ast.literal_eval(ingredient)\n",
        "    \n",
        "    ingredients = ','.join(ingredients)\n",
        "    ingredients = unidecode.unidecode(ingredients)\n",
        "    return ingredients\n",
        "\n",
        "def title_parser(title):\n",
        "    title = unidecode.unidecode(title)\n",
        "    return title \n",
        "def get_recommendations(N, scores):\n",
        "    \"\"\"\n",
        "    Rank scores and output a pandas data frame containing all the details of the top N recipes.\n",
        "    :param scores: list of cosine similarities\n",
        "    \"\"\"\n",
        "    # load in recipe dataset\n",
        "    df_recipes = pd.read_csv('/content/drive/MyDrive/RAW_recipes.csv')\n",
        "    # order the scores with and filter to get the highest N scores\n",
        "    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]\n",
        "    # create dataframe to load in recommendations\n",
        "    recommendation = pd.DataFrame(columns=[\"recipe\", \"ingredients\", \"score\", \"url\"])\n",
        "    count = 0\n",
        "    for i in top:\n",
        "        recommendation.at[count, \"recipe\"] = title_parser(df_recipes[\"name\"][i])\n",
        "        recommendation.at[count, \"ingredients\"] = ingredient_parser_final(\n",
        "            df_recipes[\"ingredients\"][i]\n",
        "        )\n",
        "        # recommendation.at[count, \"url\"] = df_recipes[\"recipe_urls\"][i]\n",
        "        recommendation.at[count, \"score\"] = f\"{scores[i]}\"\n",
        "        recommendation.at[count, \"id\"] = df_recipes[\"id\"][i]\n",
        "        count += 1\n",
        "    return recommendation\n",
        "\n",
        "def get_recs(ingredients, N=5, mean=False):\n",
        "    \"\"\"\n",
        "    Get the top N recipe recomendations.\n",
        "    :param ingredients: comma seperated string listing ingredients\n",
        "    :param N: number of recommendations\n",
        "    :param mean: False if using tfidf weighted embeddings, True if using simple mean\n",
        "    \"\"\"\n",
        "    # load in word2vec model\n",
        "    model = model_cbow\n",
        "    # normalize embeddings\n",
        "    model.init_sims(replace=True)\n",
        "    if model:\n",
        "        print(\"Successfully loaded model\")\n",
        "    # load in data\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/RAW_recipes.csv\")\n",
        "    # parse ingredients\n",
        "    data[\"parsed\"] = data.ingredients.apply(ingredient_parser)\n",
        "    # create corpus\n",
        "    corpus = get_and_sort_corpus(data)\n",
        "\n",
        "    if mean:\n",
        "        # get average embdeddings for each document\n",
        "        mean_vec_tr = MeanEmbeddingVectorizer(model)\n",
        "        doc_vec = mean_vec_tr.transform(corpus)\n",
        "        doc_vec = [doc.reshape(1, -1) for doc in doc_vec]\n",
        "        assert len(doc_vec) == len(corpus)\n",
        "    else:\n",
        "        # use TF-IDF as weights for each word embedding\n",
        "        tfidf_vec_tr = TfidfEmbeddingVectorizer(model)\n",
        "        tfidf_vec_tr.fit(corpus)\n",
        "        doc_vec = tfidf_vec_tr.transform(corpus)\n",
        "        doc_vec = [doc.reshape(1, -1) for doc in doc_vec]\n",
        "        assert len(doc_vec) == len(corpus)\n",
        "\n",
        "    # create embeddings for input text\n",
        "    input = ingredients\n",
        "    # create tokens with elements\n",
        "    input = input.split(\",\")\n",
        "    # parse ingredient list\n",
        "    input = ingredient_parser(input)\n",
        "    # get embeddings for ingredient doc\n",
        "    if mean:\n",
        "        input_embedding = mean_vec_tr.transform([input])[0].reshape(1, -1)\n",
        "    else:\n",
        "        input_embedding = tfidf_vec_tr.transform([input])[0].reshape(1, -1)\n",
        "\n",
        "    # get cosine similarity between input embedding and all the document embeddings\n",
        "    cos_sim = map(lambda x: cosine_similarity(input_embedding, x)[0][0], doc_vec)\n",
        "    scores = list(cos_sim)\n",
        "    # Filter top N recommendations\n",
        "    recommendations = get_recommendations(N, scores)\n",
        "    return recommendations\n",
        "  \n",
        "if __name__ == \"__main__\":\n",
        "    # test\n",
        "    input = 'winter squash, mexican seasoning, mixed spice, honey, butter, olive oil, salt'\n",
        "    rec = get_recs(input)\n",
        "    print(rec)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
